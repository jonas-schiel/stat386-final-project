[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Wrangling And Visual",
    "section": "",
    "text": "Introduction\nHaving usable, clean data is essential part of the data science process. In this tutorial I will teach some basic data wrangling steps and teach how users how to create a few simple visuals for that data. This blog will help students and other users understand the importance of data wrangling as well as introduce them to simple data wrangling steps. I will be using data from the Los Angeles Lakers trying to answer the question of if home court advantage really helps teams win.\n\n\nImporting Data\nThe very first thing that we need to do in answering the question of if home court advantage really helps teams win is to get clean and usable data that will show us trends and help us come up with an answer. The data set that I found contains quite a bit of information, such as points scored, field goals made, and free throw percentage. However, in this tutorial we are not interested in those variables. As part of data wrangling we can make the data a whole lot cleaner simply by eliminating the variables that we are not interested in. When reading in data using the “pd.read_csv()” function we can use the built-in feature of “usecols =” to select the columns from the dataset that we are interested in. Here you can see that I selected to use the columns “MATCH UP”, “GAME DATE”, and “W/L”\n\n\nCode\nimport pandas as pd, json\n\ncols = [\"MATCH UP\", \"GAME DATE\", \"W/L\"]\ndata = pd.read_csv(\"Lakers2009-2021.csv\", usecols = cols)\nprint(data.head())\n\n\n      MATCH UP   GAME DATE W/L\n0    LAL @ NOP  05/16/2021   W\n1    LAL @ IND  05/15/2021   W\n2  LAL vs. HOU  05/12/2021   W\n3  LAL vs. NYK  05/11/2021   W\n4  LAL vs. PHX  05/09/2021   W\n\n\n\n\nData Wrangling\nAfter importing the data I need to do some data wrangling in order to do any anlysis towards the main question. I renamed the columns using the function “data.rename” for simplicity. After renaming the columns it was easier to run it through a function to seperate Home and Away games. To do this I created the “home_away” fuction which goes through the columns and if the designated string is in the value it assigns either “Home” or “Away”. I then applied this to the data and created a new column so that I can easily see whether the game was at home or if it was away.\n\n\nCode\ndata.rename(columns = {\n    'MATCH\\xa0UP' : 'Match_Up',\n    'GAME\\xa0DATE' : 'Game_Date'\n}, inplace=True)\n\ndef home_away(x):\n    x = x.strip().lower()\n    if \"vs.\" in x:\n        return \"Home\"\n    elif \"@\" in x: \n        return \"Away\"\n\ndata[\"Home/Away\"] = data[\"Match_Up\"].apply(home_away)\n\nprint(data.head())\n\n\n      Match_Up   Game_Date W/L Home/Away\n0    LAL @ NOP  05/16/2021   W      Away\n1    LAL @ IND  05/15/2021   W      Away\n2  LAL vs. HOU  05/12/2021   W      Home\n3  LAL vs. NYK  05/11/2021   W      Home\n4  LAL vs. PHX  05/09/2021   W      Home\n\n\n\n\nData Analysis and Visualization\nAfter doing some basic data wrangling, I did some simple data analysis and created a visual to help understand the data. Since I am interested in learning whether home court advantage exists for this basketball team I calculated the win percentages at both home and away. So we can see in the code output that the team wins at home 55 percent of the time and wins on the road 41 percent of the time. The visual that I created shows both the wins and losses at home and away so viewers can see the difference and compare. This visual helps us to see quickly that the team does win more games when playing at home and we can see that percentage that we just calculated.\n\n\nCode\nimport matplotlib.pyplot as plt\nhome_wins = data[(data['Home/Away'] == 'Home') & (data['W/L'] == 'W')]\nhome_total = data[data['Home/Away'] == 'Home']\nhome_win_pct = len(home_wins) / len(home_total)\nprint(home_win_pct)\n\naway_wins = data[(data['Home/Away'] == 'Away') & (data['W/L'] == 'W')]\naway_total = data[data['Home/Away'] == 'Away']\naway_win_pct = len(away_wins) / len(away_total)\nprint(away_win_pct)\n\n\nsummary = pd.crosstab(data['Home/Away'], data['W/L'])\nsummary.plot(kind='bar', figsize=(8,5), color=['lightcoral', 'mediumseagreen'])\nplt.title('Results for Home and Away Games')\nplt.xlabel(None)\nplt.ylabel('Number of Games')\n\n\n0.5539112050739958\n0.41139240506329117\n\n\nText(0, 0.5, 'Number of Games')\n\n\n\n\n\n\n\n\n\n\n\nConclusion and Call to Action\nAfter doing just some basic data wrangling, analysis and visualization we can see that the team does win more when they are playing at home, however just with this basic level of anaysis we cannot make any strong conclusions that playing at home will lead to more wins than playing away. In order to form that conclusion we would need to do a more in depth analysis. The main takeaway for readers should be that there are simple ways to import, clean, analyze and visualize data that can be done with a few simple functions and packages. People interested in data can apply this same process by following the steps and even utalizing some of the code shown here to help them understand the data that they are working on.",
    "crumbs": [
      "Home",
      "Data Wrangling And Visual"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nHome Page\n\nIntroduction\nHi! My name is Sam Kirkham, I am a statistics major at BYU! A few things about me: I am from Kaysville Ut, I love the outdoors, and I love anything and everything sports. I love watching basketball and football and love working on data for both of these sports as well.\nTwo things I have loved learning about are data cleaning and visualization. In this blog I want to share an example as I teach a little bit about this process using sports data. In this example I am looking at the Los Angeles Lakers and whether playing at home or on the road impacts the results of their games. I hope you enjoy!",
    "crumbs": [
      "Home",
      "Home Page"
    ]
  },
  {
    "objectID": "Package_Demo.html",
    "href": "Package_Demo.html",
    "title": "Demonstration of the Functions:",
    "section": "",
    "text": "/var/folders/60/p3sjh3zx3793rb8gcc5_7qn00000gn/T/ipykernel_41557/3642518538.py:203: DtypeWarning: Columns (4,7) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('economic_election_data.csv')\n\n\nThis function, wrangle_data, allows us to wrangle downloadable csvs about economic data. Here we plug in our unemployment rate dataset and it does all of the wrangling and formatting for us. Here are just the first five rows of what the data will look like afterwards.\n\n\nCode\nUNRATE = wrangle_data(\n    filepath='UNRATE.csv',\n    value_col='UNRATE'\n)\n\nUNRATE.head()\n\n\n\n\n\n\n\n\n\nMonth\nYear\nUNRATE\n\n\n\n\n0\nJanuary\n1948\n3.4\n\n\n1\nFebruary\n1948\n3.8\n\n\n2\nMarch\n1948\n4.0\n\n\n3\nApril\n1948\n3.9\n\n\n4\nMay\n1948\n3.5\n\n\n\n\n\n\n\nThe next function, merge_and_sort_data, combines all of the economic data that we gathered into one data set. Here you can see that all three of the data sets are combined together to make one dataset. There are a few missing values but those get taken care of in another function that we have.\n\n\nCode\ncombined_data = merge_and_sort_data(UNRATE, GDP, CPI)\ncombined_data.head()\n\n\n\n\n\n\n\n\n\nMonth\nYear\nUNRATE\nGDP\nCPI\n\n\n\n\n0\nJanuary\n1948\n3.4\n265.742\n23.68\n\n\n1\nFebruary\n1948\n3.8\nNaN\n23.67\n\n\n2\nMarch\n1948\n4.0\nNaN\n23.50\n\n\n3\nApril\n1948\n3.9\n272.567\n23.82\n\n\n4\nMay\n1948\n3.5\nNaN\n24.01\n\n\n\n\n\n\n\nOur next fuction, get_presidents_html, allows us to scrape the data about presidents of the United States, the years that they were president and the political party that they represented. This allows for easier data combination and cleaning so we will combine that with the last function, add_presidnets_to_data, we combine the data that we have and clean it creating our final president and economic dataset ready to be combined with the election data.\n\n\nCode\nurl = 'https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States' \nemail = \"skirk03@byu.edu\" \nua = f\"STAT386-class-scraper/1.0 (+{email})\" \nr = requests.get(url, headers={\"User-Agent\": ua, \"From\": email}, timeout=15)\n\ntext = get_presidents_html(ua = f\"STAT386-class-scraper/1.0 (+{email})\", email = \"skirk03@byu.edu\")\n\nFinal_df = add_presidents_to_data(combined_data, text)\nFinal_df.head(20)\n\n\n/var/folders/60/p3sjh3zx3793rb8gcc5_7qn00000gn/T/ipykernel_41557/3642518538.py:50: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  tables = pd.read_html(html_text)\n/var/folders/60/p3sjh3zx3793rb8gcc5_7qn00000gn/T/ipykernel_41557/3642518538.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df[\"Start_Date\"] = pd.to_datetime(\n/var/folders/60/p3sjh3zx3793rb8gcc5_7qn00000gn/T/ipykernel_41557/3642518538.py:71: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df[\"End_Date\"] = pd.to_datetime(\n\n\n\n\n\n\n\n\n\nMonth\nYear\nUNRATE\nGDP\nCPI\nPresident\nParty\n\n\n\n\n0\nJanuary\n1948\n3.4\n265.742000\n23.68\nHarry S Truman\nDemocratic\n\n\n2\nFebruary\n1948\n3.8\n268.017000\n23.67\nHarry S Truman\nDemocratic\n\n\n4\nMarch\n1948\n4.0\n270.292000\n23.50\nHarry S Truman\nDemocratic\n\n\n6\nApril\n1948\n3.9\n272.567000\n23.82\nHarry S Truman\nDemocratic\n\n\n8\nMay\n1948\n3.5\n274.776667\n24.01\nHarry S Truman\nDemocratic\n\n\n10\nJune\n1948\n3.6\n276.986333\n24.15\nHarry S Truman\nDemocratic\n\n\n12\nJuly\n1948\n3.6\n279.196000\n24.40\nHarry S Truman\nDemocratic\n\n\n14\nAugust\n1948\n3.9\n279.586000\n24.43\nHarry S Truman\nDemocratic\n\n\n16\nSeptember\n1948\n3.8\n279.976000\n24.36\nHarry S Truman\nDemocratic\n\n\n18\nOctober\n1948\n3.7\n280.366000\n24.31\nHarry S Truman\nDemocratic\n\n\n20\nNovember\n1948\n3.8\n278.588667\n24.16\nHarry S Truman\nDemocratic\n\n\n22\nDecember\n1948\n4.0\n276.811333\n24.05\nHarry S Truman\nDemocratic\n\n\n24\nJanuary\n1949\n4.3\n275.034000\n24.01\nHarry S Truman\nDemocratic\n\n\n26\nFebruary\n1949\n4.7\n273.806333\n23.91\nHarry S Truman\nDemocratic\n\n\n28\nMarch\n1949\n5.0\n272.578667\n23.91\nHarry S Truman\nDemocratic\n\n\n30\nApril\n1949\n5.3\n271.351000\n23.92\nHarry S Truman\nDemocratic\n\n\n32\nMay\n1949\n6.1\n271.863667\n23.91\nHarry S Truman\nDemocratic\n\n\n34\nJune\n1949\n6.2\n272.376333\n23.92\nHarry S Truman\nDemocratic\n\n\n36\nJuly\n1949\n6.7\n272.889000\n23.70\nHarry S Truman\nDemocratic\n\n\n38\nAugust\n1949\n6.8\n272.135000\n23.70\nHarry S Truman\nDemocratic\n\n\n\n\n\n\n\nOur last function does some simple eda for us to show us how the ecomonic features impact the voting percentages and creates a correlation plot that allows us to easily the factors with the most impact. Those plots are here below.\n\n\nCode\nsimple_eda(df, scatter_points= False)"
  },
  {
    "objectID": "DataAcquisation.html",
    "href": "DataAcquisation.html",
    "title": "Data Acquisition Blog 2",
    "section": "",
    "text": "Introduction:\nWith the success that the BYU football team has had this season it is interesting that they have been betting underdogs for a quite a few games this season, including this last week against Iowa State, a team that had already lost two games this season. So when the betting line came out that BYU is a 10.5 point underdog for this week’s game against Texas Tech I was shocked that it was such a large line. Recently I heard a theory that the lines are skewed in an effort to bring more bettors to the game because a majority of BYU fans do not gamble. So I was interested in looking into this question and seeing what I could learn about BYU betting lines from the last few seasons. My hope from this project is to learn as a fan of BYU, if there is any credit to the skewed betting lines or if I should ignore them as they are soley an effort to bring more money to bets on BYU games.\n\n\nMotivating Question:\nThe question that I came up with is, what factors are good predictors for if BYU will cover the line that is set by odds makers. What this looks like is are there factors, like BYU is playing at home this weekend, or does the total number of points scored relate to BYU covering the spread given. I also have other factors such as BYU’s point differential as well as the starting spread from odds makers.\n\n\nEthical Data Acquisition:\nI found data for my question of interest on collegefootballdata.com. As I was looking at the datasets that they had available I saw they had a lot of good data for any college football team that I wanted and found the data for BYU. After finding the data that I wanted I looked at their terms and conditions to learn more about getting data from them. The data was in a format that allowed for easy downloading without signing up for anything, which was a sign that getting data from them was okay. They did have an option to sign up for an API key to get data from them as well and after looking into that further I never saw a place where using the API key was the only way to get the data. There was no mention of any restrictions in data usage.\n\n\nSummary of Getting Data:\nFrom collegefootball.com I was ale to get data for betting lines per season. In order to get enough data to come to some conclusions I grabbed data from the 2020 season through the games that have been played so far in the 2025 season. Each season’s data came as a .csv file so getting the data was not difficult as it just required downloading each file and then reading them in using pd.read_csv. In order to create a data set that included all of these seasons I had to use pd.concat. After doing that I had a nice data set that included all the data. If someone wanted to follow a similar path of combining .csv files together to make a data set I think that pd.concat is the easiest way as it will take the columns that are included in the individual sets and all of those columns will be included in the final data set, any missing data will be filled in with NaNs.\nFor my question of interest I wanted to extract some data that was already included in the data and add additional columns that would help with the analysis. Most of these new columns were simple as they were things like adding two of the original columns or creating boolean statements that would create columns that are true or false values. After creating these columns of interest I dropped any columns that I will not be using to ensure that my data is neat and clean. These are simple steps that can be done to any dataset that allows for a more in depth analysis.\n\n\nEDA Highlights:\n\n\nCode\nimport pandas as pd, json\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\n\ndata = pd.read_csv('BettingLinesFinal.csv')\ndata.head()\n\nlen(data)\n\n\n251\n\n\nYou can see that I have 251 observations. These come from the games that BYU has played in the last 6 seasons.\n\n\nCode\ndata.describe()\n\n\n\n\n\n\n\n\n\nHomeScore\nAwayScore\nOverUnder\nYear\nNormalizedSpread\nBYU_Diff\nTotalScore\nOver/Under\n\n\n\n\ncount\n251.000000\n251.000000\n249.000000\n251.000000\n251.000000\n251.000000\n251.000000\n249.000000\n\n\nmean\n30.247012\n25.163347\n54.871486\n2022.027888\n6.500000\n9.553785\n55.410359\n0.481928\n\n\nstd\n12.679540\n12.707526\n7.296676\n1.618400\n13.569598\n19.143670\n14.571580\n13.308857\n\n\nmin\n3.000000\n0.000000\n39.500000\n2020.000000\n-24.500000\n-33.000000\n14.000000\n-36.500000\n\n\n25%\n21.500000\n17.000000\n49.500000\n2021.000000\n-3.500000\n-3.000000\n45.500000\n-7.000000\n\n\n50%\n28.000000\n24.000000\n54.500000\n2022.000000\n3.500000\n9.000000\n55.000000\n0.500000\n\n\n75%\n38.000000\n34.000000\n60.000000\n2023.000000\n14.500000\n20.000000\n62.000000\n8.500000\n\n\nmax\n69.000000\n55.000000\n80.000000\n2025.000000\n50.000000\n69.000000\n115.000000\n49.000000\n\n\n\n\n\n\n\nJust some basic summary statistics from the data set.\n\n\nCode\nsns.countplot(data = data, x = 'Year', hue = 'Cover')\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot shows the number of spreads that BYU has covered for each of the last 6 football seasons. From this graph you can clearly see the years that BYU football has had the most success.\n\n\nCode\nbyu_underdog = data[data['NormalizedSpread'] &lt; 0]\nbyu_underdog_25 = byu_underdog[byu_underdog['Year'].isin([2024, 2025])]\nnum_underdog_covered = byu_underdog_25['Cover'].sum()\n\nprint(f\"BYU was an underdog in {len(byu_underdog_25)} games.\")\nprint(f\"BYU covered the spread in {num_underdog_covered} of those games.\")\n\n\nBYU was an underdog in 24 games.\nBYU covered the spread in 21 of those games.\n\n\nI was especially interested in the last 2 BYU football seasons so I looked, got just the data for that and found the games where BYU was expected to lose. Here are the results.\n\n\nLinks to Resources:\nLink For The Data\nLink For More Code",
    "crumbs": [
      "Home",
      "Data Acquisition Blog 2"
    ]
  },
  {
    "objectID": "Final.html",
    "href": "Final.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nIntroduction:\nFor this project we were curious about what things impact elections. We gathered some election data as well as economic data and did some basic exploratory data analysis to come up with some interesting findings.\n\n\nData Collection:\nGetting our data was quite the process. We gathered data from a bunch of different sources and then combined them all together to make our dataset. We scraped some tables and that presented a few challenges especially with the election data that we have because each of the tables was formatted a little differently from each other, so we had to hard code the scraping for each table. There were also some parts of the data that we gathered that were simple csvs that just needed to be downloaded and wrangled. So after we gathered all of the data we wrangled them so that they could all be easily merged together. Ultimately we have created a data set that includes election data from 1948 onwards as well as the GDP, CPI and unemployment rates for every month from 1948 until now as well. We wanted to gather this data to see if we could do an interesting study on predictors for which political party would be elected.\n\n\nData Wrangling:\nThere was a lot of wrangling that we needed to do for this data set. There were three small downloadable csvs that we got and the wrangling for those mostly consisted of renaming columns, and calculating a few things to make new columns. The larger part of the wrangling came with the presidents and election portions of the dataset. Both of these had some missing data that needed to be filled in. We couldn’t used a mean or median imputation for filling those values either as some of them were just unknown or were president names that didn’t get scraped properly. In order to fix that we needed to be very specific in filling in those values. The thing that worked best was actually just to create some lists and then merge that with the data set in order to fill in those missing values. Then for the last part of the wrangling we had to make sure that the data had similar columns and formats so that we could merge them properly."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nAbout Me\n\n\n\nPicture of Me\n\n\n\nEducation\n\nBrigham Young University\n\nMajoring in Applied Statistics\nProjected Graduation April 2026\n\nRelevant Course Work\n\nData Science Process\nPredictive Analytics\nApplied R Programming\nSpreadsheets and Business Analysis\n\n\n\n\nExperience\n\nTeacher, Missionary Training Center\n\nEducate young adults in teaching skills\nLead groups of 10 - 14 young adults in practices of new skills\n\nCounselor, For the Strength of Youth\n\nLead groups of 14 - 18 youth in summer camp activities\nTeach short lessons to small groups\n\nProjects\n\nAmazon Acess\n\nI am currently working on different models to see how accurately I can predict who gets access in Amazon’s system\n\nNBA 2024 Draft Picks\n\nI did some initial EDA and modeling to notice any trends in player stats that would help predict which players would be drafted\n\n\n\n\n\nSkills\n\nCoding Languages\n\nR\nPython\n\nOther Skills\n\nExcel Spreadsheets\nCommunication\nTeaching\n\n\n\n\nGet to Know Me\nI love anything and everything outdoors. One of my favorite hobbies is disc golf, I love that it gets me outdoors and into the mountains. I also love soccer, I have played soccer since I was a little kid. I’ve never played in high school or college, I’ve only ever played for fun in recreation leagues and competitive leagues. Just some small fun facts about me: I served a mission for the Church of Jesus Christ of Latter-Day Saints in Atlanta Georgia, I love Dr. Pepper and despite going to BYU my favorite college football team is the Georgia Football team.",
    "crumbs": [
      "Home",
      "About Me"
    ]
  }
]